{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd296b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a2bca24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ming'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem('mingment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e26ee865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ing']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5af67366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processing']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac2e8523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'ing')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d536fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('processe', 's')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8256e302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'es')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34adf6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', '')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0badfae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fddd804e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mehonja'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(\"mehonjaed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c0da234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'women',\n",
       " 'ly',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'i',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government. Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "[stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14cf994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8eb17c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nervous; sensible; poorer; young; single; thriving; young; young;\n",
      "sensible; better; young; young; young; sensible; young; married;\n",
      "young; young; tall; young; young; gallant; sensible; young; young;\n",
      "fortunate; sensible\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "emma = nltk.Text(gutenberg.words('austen-emma.txt'))\n",
    "emma.findall(r\"<a> (<.*>) <man>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90f93b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you rule bro; telling you bro; u twizted bro\n"
     ]
    }
   ],
   "source": [
    "chat = nltk.Text(nps_chat.words())\n",
    "chat.findall(r\"<.*> <.*> <bro>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dcac323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c53113e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'women',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'ponds',\n",
       " 'distributing',\n",
       " 'swords',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basis',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'government',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'executive',\n",
       " 'power',\n",
       " 'derives',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'masses',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government. Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5377d7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denni',\n",
       " ':',\n",
       " 'listen',\n",
       " ',',\n",
       " 'strang',\n",
       " 'women',\n",
       " 'lie',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandat',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcic',\n",
       " 'aquat',\n",
       " 'ceremoni',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e768e231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['den',\n",
       " ':',\n",
       " 'list',\n",
       " ',',\n",
       " 'strange',\n",
       " 'wom',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'bas',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'pow',\n",
       " 'der',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mand',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'som',\n",
       " 'farc',\n",
       " 'aqu',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a741fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'woman',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distributing',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basis',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'government',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'executive',\n",
       " 'power',\n",
       " 'derives',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0acd4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b011705c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone\\nthough),',\n",
       " \"'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very\\nwell',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r' ', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c3284d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though),',\n",
       " \"'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'[ \\t\\n]+', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3d6921d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'When',\n",
       " 'I',\n",
       " 'M',\n",
       " 'a',\n",
       " 'Duchess',\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " 'not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though',\n",
       " 'I',\n",
       " 'won',\n",
       " 't',\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without',\n",
       " 'Maybe',\n",
       " 'it',\n",
       " 's',\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot',\n",
       " 'tempered',\n",
       " '']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\W+', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37588674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " 'I',\n",
       " \"'M\",\n",
       " 'a',\n",
       " 'Duchess',\n",
       " ',',\n",
       " \"'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " ',',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though',\n",
       " ')',\n",
       " ',',\n",
       " \"'I\",\n",
       " 'won',\n",
       " \"'t\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL',\n",
       " '.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without',\n",
       " '-',\n",
       " '-Maybe',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot',\n",
       " '-tempered',\n",
       " ',',\n",
       " \"'\",\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\w+|\\S\\w*', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "423c3ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65b6a5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '', ''), ('', '', ''), ('', '-print', ''), ('', '', ''), ('', '', '')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x) # set flag to allow verbose regexps\n",
    "... ([A-Z]\\.)+ # abbreviations, e.g. U.S.A.\n",
    "... | \\w+(-\\w+)* # words with optional internal hyphens\n",
    "... | \\$?\\d+(\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "... | \\.\\.\\. # ellipsis\n",
    "... | [][.,;\"'?():-_`] # these are separate tokens\n",
    "... '''\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8418c89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.250994070456922"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02ed173b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the wild events which were to follow this girl had no\\n'\n",
      " 'part at all; he never saw her again until all his tale was over.',\n",
      " 'And yet, in some indescribable way, she kept recurring like a\\n'\n",
      " 'motive in music through all his mad adventures afterwards, and the\\n'\n",
      " 'glory of her strange hair ran like a red thread through those dark\\n'\n",
      " 'and ill-drawn tapestries of the night.',\n",
      " 'For what followed was so\\nimprobable, that it might well have been a dream.',\n",
      " 'When Syme went out into the starlit street, he found it for the\\n'\n",
      " 'moment empty.',\n",
      " 'Then he realised (in some odd way) that the silence\\n'\n",
      " 'was rather a living silence than a dead one.',\n",
      " 'Directly outside the\\n'\n",
      " 'door stood a street lamp, whose gleam gilded the leaves of the tree\\n'\n",
      " 'that bent out over the fence behind him.',\n",
      " 'About a foot from the\\n'\n",
      " 'lamp-post stood a figure almost as rigid and motionless as the\\n'\n",
      " 'lamp-post itself.',\n",
      " 'The tall hat and long frock coat were black; the\\n'\n",
      " 'face, in an abrupt shadow, was almost as dark.',\n",
      " 'Only a fringe of\\n'\n",
      " 'fiery hair against the light, and also something aggressive in the\\n'\n",
      " 'attitude, proclaimed that it was the poet Gregory.',\n",
      " 'He had something\\n'\n",
      " 'of the look of a masked bravo waiting sword in hand for his foe.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = sent_tokenizer.tokenize(text)\n",
    "pprint.pprint(sents[171:181])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0635eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "167dd12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b0d74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04f95dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "segment(text, seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8abb193e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do',\n",
       " 'you',\n",
       " 'see',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'see',\n",
       " 'the',\n",
       " 'doggy',\n",
       " 'do',\n",
       " 'you',\n",
       " 'like',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'like',\n",
       " 'the',\n",
       " 'doggy']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(text, seg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6aee4b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We called him Tortoise because he taught us .'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n",
    "' '.join(silly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cbd7809b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We;called;him;Tortoise;because;he;taught;us;.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "';'.join(silly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7030523c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WemehocalledmehohimmehoTortoisemehobecausemehohemehotaughtmehousmeho.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'meho'.join(silly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "de171072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog -> 4 ;\n",
      "cat -> 3 ;\n",
      "snake -> 1 ;\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
    "for word in fdist:\n",
    "    print(word, '->', fdist[word], ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6fdd4c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog->4;\n",
      "cat->3;\n",
      "snake->1;\n"
     ]
    }
   ],
   "source": [
    "for word in fdist:\n",
    "    print('%s->%d;' % (word, fdist[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2c53c1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat->3;'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%s->%d;' % ('cat', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9df815aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m->\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "'%s->%d;' % 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d52a0b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat->'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%s->' % 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6419f2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%d' % 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4bbae49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lee wants a sandwich for lunch'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"%s wants a %s %s\" % (\"Lee\", \"sandwich\", \"for lunch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa099bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee wants a sandwich right now\n",
      "Lee wants a spam fritter right now\n",
      "Lee wants a pancake right now\n"
     ]
    }
   ],
   "source": [
    "template = 'Lee wants a %s right now'\n",
    "menu = ['sandwich', 'spam fritter', 'pancake']\n",
    "for snack in menu:\n",
    "    print(template % snack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "142af4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gore-dole -> 2 ; meho -> 1 ; sarma -> 1 ; sarmuth -> 1 ; hamdija -> 1 ; "
     ]
    }
   ],
   "source": [
    "# meho = ['meho', 'sarma', 'sarmuth', 'hamdija', 'gore-dole']\n",
    "fdist = nltk.FreqDist(['meho', 'sarma', 'sarmuth', 'hamdija', 'gore-dole', 'gore-dole'])\n",
    "for i in fdist:\n",
    "    print(i, '->', fdist[i],';', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2dcfcb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog -> 4 ;\n",
      "cat -> 3 ;\n",
      "snake -> 1 ;\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
    "for word in fdist:\n",
    "    print(word, '->', fdist[word], ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a96ca563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog->4;\n",
      "cat->3;\n",
      "snake->1;\n"
     ]
    }
   ],
   "source": [
    "for word in fdist:\n",
    "    print('%s->%d;' % (word, fdist[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a0a2e06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   dog'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%6s' % 'dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3f93c736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog   '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%-6s' % 'dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915bb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "314ffa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meho               '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%-*s' % (width, 'meho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "221f821d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy for 9375 words: 34.186667%'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, total = 3205, 9375\n",
    "\"accuracy for %d words: %2.6f%%\" % (total, 100 * count / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e5b2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tabulate(cfdist, words, categories):\n",
    "    print('%-16s' % 'Category')\n",
    "    for word in words: # column headings\n",
    "        print('%6s' % word)\n",
    "    print(',')\n",
    "    for category in categories:\n",
    "        print('%-16s' % category) # row heading\n",
    "        for word in words: # for each word\n",
    "            print('%6d' % cfdist[category][word]) # print table cell\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc7848b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open('output.txt', 'w')\n",
    "words = set(nltk.corpus.genesis.words('english-kjv.txt'))\n",
    "for word in sorted(words):\n",
    "    output_file.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e7d4216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2789"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1c1deb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2789'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3e7de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_file.write(str(len(words)) + \"\\n\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5169872a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5),\n",
      "all (3),\n",
      "is (2),\n",
      "said (4),\n",
      "and (3),\n",
      "done (4),\n",
      ", (1),\n",
      "more (4),\n",
      "is (2),\n",
      "said (4),\n",
      "than (4),\n",
      "done (4),\n",
      ". (1),\n"
     ]
    }
   ],
   "source": [
    "\n",
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n",
    "'more', 'is', 'said', 'than', 'done', '.']\n",
    "for word in saying:\n",
    "    print(word, '(' + str(len(word)) + '),')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63c9f7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more\n",
      "(4), is (2), said (4), than (4), done (4), . (1),\n"
     ]
    }
   ],
   "source": [
    "from textwrap import fill\n",
    "format = '%s (%d),'\n",
    "pieces = [format % (word, len(word)) for word in saying]\n",
    "output = ' '.join(pieces)\n",
    "wrapped = fill(output)\n",
    "print(wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e87773f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE\n",
      "porter: the\n",
      "lancaster: the\n",
      "Dawn\n",
      "porter: dawn\n",
      "lancaster: dawn\n",
      "of\n",
      "porter: of\n",
      "lancaster: of\n",
      "Love\n",
      "porter: love\n",
      "lancaster: lov\n",
      "is\n",
      "porter: is\n",
      "lancaster: is\n",
      "an\n",
      "porter: an\n",
      "lancaster: an\n",
      "oil\n",
      "porter: oil\n",
      "lancaster: oil\n",
      "painting\n",
      "porter: paint\n",
      "lancaster: paint\n",
      "by\n",
      "porter: by\n",
      "lancaster: by\n",
      "English\n",
      "porter: english\n",
      "lancaster: engl\n",
      "artist\n",
      "porter: artist\n",
      "lancaster: art\n",
      "William\n",
      "porter: william\n",
      "lancaster: william\n",
      "Etty\n",
      "porter: etti\n",
      "lancaster: etty\n",
      ",\n",
      "porter: ,\n",
      "lancaster: ,\n",
      "first\n",
      "porter: first\n",
      "lancaster: first\n",
      "exhibited\n",
      "porter: exhibit\n",
      "lancaster: exhibit\n",
      "in\n",
      "porter: in\n",
      "lancaster: in\n",
      "1828\n",
      "porter: 1828\n",
      "lancaster: 1828\n",
      ".\n",
      "porter: .\n",
      "lancaster: .\n",
      "Loosely\n",
      "porter: loos\n",
      "lancaster: loos\n",
      "based\n",
      "porter: base\n",
      "lancaster: bas\n",
      "on\n",
      "porter: on\n",
      "lancaster: on\n",
      "a\n",
      "porter: a\n",
      "lancaster: a\n",
      "passage\n",
      "porter: passag\n",
      "lancaster: pass\n",
      "from\n",
      "porter: from\n",
      "lancaster: from\n",
      "John\n",
      "porter: john\n",
      "lancaster: john\n",
      "Milton\n",
      "porter: milton\n",
      "lancaster: milton\n",
      "'s\n",
      "porter: 's\n",
      "lancaster: 's\n",
      "1634\n",
      "porter: 1634\n",
      "lancaster: 1634\n",
      "Comus\n",
      "porter: comu\n",
      "lancaster: com\n",
      ",\n",
      "porter: ,\n",
      "lancaster: ,\n",
      "it\n",
      "porter: it\n",
      "lancaster: it\n",
      "shows\n",
      "porter: show\n",
      "lancaster: show\n",
      "Venus\n",
      "porter: venu\n",
      "lancaster: ven\n",
      "leaning\n",
      "porter: lean\n",
      "lancaster: lean\n",
      "across\n",
      "porter: across\n",
      "lancaster: across\n",
      "to\n",
      "porter: to\n",
      "lancaster: to\n",
      "wake\n",
      "porter: wake\n",
      "lancaster: wak\n",
      "the\n",
      "porter: the\n",
      "lancaster: the\n",
      "sleeping\n",
      "porter: sleep\n",
      "lancaster: sleep\n",
      "Love\n",
      "porter: love\n",
      "lancaster: lov\n",
      "by\n",
      "porter: by\n",
      "lancaster: by\n",
      "stroking\n",
      "porter: stroke\n",
      "lancaster: stroking\n",
      "his\n",
      "porter: hi\n",
      "lancaster: his\n",
      "wings\n",
      "porter: wing\n",
      "lancaster: wing\n",
      ".\n",
      "porter: .\n",
      "lancaster: .\n",
      "It\n",
      "porter: it\n",
      "lancaster: it\n",
      "was\n",
      "porter: wa\n",
      "lancaster: was\n",
      "very\n",
      "porter: veri\n",
      "lancaster: very\n",
      "poorly\n",
      "porter: poorli\n",
      "lancaster: poor\n",
      "received\n",
      "porter: receiv\n",
      "lancaster: receiv\n",
      "when\n",
      "porter: when\n",
      "lancaster: when\n",
      "first\n",
      "porter: first\n",
      "lancaster: first\n",
      "exhibited\n",
      "porter: exhibit\n",
      "lancaster: exhibit\n",
      ";\n",
      "porter: ;\n",
      "lancaster: ;\n",
      "the\n",
      "porter: the\n",
      "lancaster: the\n",
      "stylised\n",
      "porter: stylis\n",
      "lancaster: styl\n",
      "Venus\n",
      "porter: venu\n",
      "lancaster: ven\n",
      "was\n",
      "porter: wa\n",
      "lancaster: was\n",
      "thought\n",
      "porter: thought\n",
      "lancaster: thought\n",
      "unduly\n",
      "porter: unduli\n",
      "lancaster: undu\n",
      "influenced\n",
      "porter: influenc\n",
      "lancaster: influ\n",
      "by\n",
      "porter: by\n",
      "lancaster: by\n",
      "foreign\n",
      "porter: foreign\n",
      "lancaster: foreign\n",
      "artists\n",
      "porter: artist\n",
      "lancaster: art\n",
      "such\n",
      "porter: such\n",
      "lancaster: such\n",
      "as\n",
      "porter: as\n",
      "lancaster: as\n",
      "Rubens\n",
      "porter: ruben\n",
      "lancaster: rub\n",
      "as\n",
      "porter: as\n",
      "lancaster: as\n",
      "well\n",
      "porter: well\n",
      "lancaster: wel\n",
      "as\n",
      "porter: as\n",
      "lancaster: as\n",
      "being\n",
      "porter: be\n",
      "lancaster: being\n",
      "overly\n",
      "porter: overli\n",
      "lancaster: ov\n",
      "voluptuous\n",
      "porter: voluptu\n",
      "lancaster: voluptu\n",
      "and\n",
      "porter: and\n",
      "lancaster: and\n",
      "unrealistically\n",
      "porter: unrealist\n",
      "lancaster: unr\n",
      "coloured\n",
      "porter: colour\n",
      "lancaster: colo\n",
      ",\n",
      "porter: ,\n",
      "lancaster: ,\n",
      "while\n",
      "porter: while\n",
      "lancaster: whil\n",
      "the\n",
      "porter: the\n",
      "lancaster: the\n",
      "painting\n",
      "porter: paint\n",
      "lancaster: paint\n",
      "as\n",
      "porter: as\n",
      "lancaster: as\n",
      "a\n",
      "porter: a\n",
      "lancaster: a\n",
      "whole\n",
      "porter: whole\n",
      "lancaster: whol\n",
      "was\n",
      "porter: wa\n",
      "lancaster: was\n",
      "considered\n",
      "porter: consid\n",
      "lancaster: consid\n",
      "tasteless\n",
      "porter: tasteless\n",
      "lancaster: tasteless\n",
      "and\n",
      "porter: and\n",
      "lancaster: and\n",
      "obscene\n",
      "porter: obscen\n",
      "lancaster: obsc\n",
      ".\n",
      "porter: .\n",
      "lancaster: .\n",
      "The\n",
      "porter: the\n",
      "lancaster: the\n",
      "Dawn\n",
      "porter: dawn\n",
      "lancaster: dawn\n",
      "of\n",
      "porter: of\n",
      "lancaster: of\n",
      "Love\n",
      "porter: love\n",
      "lancaster: lov\n",
      "was\n",
      "porter: wa\n",
      "lancaster: was\n",
      "omitted\n",
      "porter: omit\n",
      "lancaster: omit\n",
      "from\n",
      "porter: from\n",
      "lancaster: from\n",
      "the\n",
      "porter: the\n",
      "lancaster: the\n",
      "major\n",
      "porter: major\n",
      "lancaster: maj\n",
      "1849\n",
      "porter: 1849\n",
      "lancaster: 1849\n",
      "retrospective\n",
      "porter: retrospect\n",
      "lancaster: retrospect\n",
      "exhibition\n",
      "porter: exhibit\n",
      "lancaster: exhibit\n",
      "of\n",
      "porter: of\n",
      "lancaster: of\n",
      "Etty\n",
      "porter: etti\n",
      "lancaster: etty\n",
      "'s\n",
      "porter: 's\n",
      "lancaster: 's\n",
      "works\n",
      "porter: work\n",
      "lancaster: work\n",
      ",\n",
      "porter: ,\n",
      "lancaster: ,\n",
      "and\n",
      "porter: and\n",
      "lancaster: and\n",
      "its\n",
      "porter: it\n",
      "lancaster: it\n",
      "exhibition\n",
      "porter: exhibit\n",
      "lancaster: exhibit\n",
      "in\n",
      "porter: in\n",
      "lancaster: in\n",
      "Glasgow\n",
      "porter: glasgow\n",
      "lancaster: glasgow\n",
      "in\n",
      "porter: in\n",
      "lancaster: in\n",
      "1899\n",
      "porter: 1899\n",
      "lancaster: 1899\n",
      "drew\n",
      "porter: drew\n",
      "lancaster: drew\n",
      "complaints\n",
      "porter: complaint\n",
      "lancaster: complaint\n",
      "for\n",
      "porter: for\n",
      "lancaster: for\n",
      "its\n",
      "porter: it\n",
      "lancaster: it\n",
      "supposed\n",
      "porter: suppos\n",
      "lancaster: suppos\n",
      "obscenity\n",
      "porter: obscen\n",
      "lancaster: obsc\n",
      ".\n",
      "porter: .\n",
      "lancaster: .\n",
      "In\n",
      "porter: in\n",
      "lancaster: in\n",
      "1889\n",
      "porter: 1889\n",
      "lancaster: 1889\n",
      "it\n",
      "porter: it\n",
      "lancaster: it\n",
      "was\n",
      "porter: wa\n",
      "lancaster: was\n",
      "bought\n",
      "porter: bought\n",
      "lancaster: bought\n",
      "by\n",
      "porter: by\n",
      "lancaster: by\n",
      "Merton\n",
      "porter: merton\n",
      "lancaster: merton\n",
      "Russell-Cotes\n",
      "porter: russell-cot\n",
      "lancaster: russell-cotes\n",
      ",\n",
      "porter: ,\n",
      "lancaster: ,\n",
      "and\n",
      "porter: and\n",
      "lancaster: and\n",
      "has\n",
      "porter: ha\n",
      "lancaster: has\n",
      "remained\n",
      "porter: remain\n",
      "lancaster: remain\n",
      "in\n",
      "porter: in\n",
      "lancaster: in\n",
      "the\n",
      "porter: the\n",
      "lancaster: the\n",
      "collection\n",
      "porter: collect\n",
      "lancaster: collect\n",
      "of\n",
      "porter: of\n",
      "lancaster: of\n",
      "the\n",
      "porter: the\n",
      "lancaster: the\n",
      "Russell-Cotes\n",
      "porter: russell-cot\n",
      "lancaster: russell-cotes\n",
      "Art\n",
      "porter: art\n",
      "lancaster: art\n",
      "Gallery\n",
      "porter: galleri\n",
      "lancaster: gallery\n",
      "&\n",
      "porter: &\n",
      "lancaster: &\n",
      "Museum\n",
      "porter: museum\n",
      "lancaster: muse\n",
      "ever\n",
      "porter: ever\n",
      "lancaster: ev\n",
      "since\n",
      "porter: sinc\n",
      "lancaster: sint\n",
      ".\n",
      "porter: .\n",
      "lancaster: .\n"
     ]
    }
   ],
   "source": [
    "# Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. \n",
    "# Do the same thing with the Lancaster Stemmer, and see if you observe any differences.  \n",
    "import nltk\n",
    "from nltk.book import *\n",
    "from nltk.corpus import genesis\n",
    "\n",
    "'''text = genesis.words()\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()'''\n",
    "\n",
    "raw = \"\"\"THE Dawn of Love is an oil painting by English artist \n",
    "William Etty, first exhibited in 1828. Loosely based on a passage \n",
    "from John Milton's 1634 Comus, it shows Venus leaning across to \n",
    "wake the sleeping Love by stroking his wings. It was very poorly \n",
    "received when first exhibited; the stylised Venus was thought unduly \n",
    "influenced by foreign artists such as Rubens as well as being overly \n",
    "voluptuous and unrealistically coloured, while the painting as a whole \n",
    "was considered tasteless and obscene. The Dawn of Love was omitted \n",
    "from the major 1849 retrospective exhibition of Etty's works, and \n",
    "its exhibition in Glasgow in 1899 drew complaints for its supposed \n",
    "obscenity. In 1889 it was bought by Merton Russell-Cotes, and has \n",
    "remained in the collection of the Russell-Cotes Art Gallery & Museum ever since.\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "\n",
    "for i in tokens:\n",
    "    print(i)\n",
    "    print(\"porter: \" + porter.stem(i))\n",
    "    print(\"lancaster: \" + lancaster.stem(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0719228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw = \"\"\"THE Dawn of Love is an oil painting by English artist \n",
    "William Etty, first exhibited in 1828. Loosely based on a passage \n",
    "from John Milton's 1634 Comus, it shows Venus leaning across to \n",
    "wake the sleeping Love by stroking his wings. It was very poorly \n",
    "received when first exhibited; the stylised Venus was thought unduly \n",
    "influenced by foreign artists such as Rubens as well as being overly \n",
    "voluptuous and unrealistically coloured, while the painting as a whole \n",
    "was considered tasteless and obscene. The Dawn of Love was omitted \n",
    "from the major 1849 retrospective exhibition of Etty's works, and \n",
    "its exhibition in Glasgow in 1899 drew complaints for its supposed \n",
    "obscenity. In 1889 it was bought by Merton Russell-Cotes, and has \n",
    "remained in the collection of the Russell-Cotes Art Gallery & Museum ever since.\"\"\"\n",
    "# from Wikipedia 2018-08-08's featured article\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "porter_output = [porter.stem(t) for t in tokens]             \n",
    "lancaster_output = [lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e86dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39db7866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bosna'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Write code to convert nationality adjectives such as Canadian and Australian to their \n",
    "#corresponding nouns Canada and Australia.\n",
    "import re\n",
    "# Argentina - Argentinian\n",
    "# Australia - Australian\n",
    "# Austria - Austrian\n",
    "# to be finished...\n",
    "\n",
    "pattern = r'(\\w+)ian'\n",
    "repl = r'\\1a' # on ovog keca repleysa-a sa (\\w) u gornjoj varijabli i dobije tu bazu te rijeci\n",
    "re.sub(pattern, repl, 'Bosnian')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "516ce418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter:  ['ovaj', 'meho', 'je', 'pravi', 'ludjak', ',', 'a', '(', 'ma', ')', 'lud', 'covjek', 'skroz', 'na', '(', 'skroz', ')', 'kad', 'ti', 'kazem', 'ba', '.']\n",
      "lancaster:  ['ovas', 'meho', 'je', 'prav', 'ludjak', ',', 'a', '(', 'ma', ')', 'lud', 'covjek', 'skroz', 'na', '(', 'skroz', ')', 'kad', 'ti', 'kazem', 'ba', '.']\n"
     ]
    }
   ],
   "source": [
    "meho = \"\"\"Ovaj Meho je pravi ludjak, a(ma) lud covjek skroz na(skroz) kad ti kazem ba.\"\"\"\n",
    "tokens = nltk.word_tokenize(meho)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "porter_stemer = [porter.stem(i) for i in tokens]\n",
    "lancaster_stemer = [lancaster.stem(i) for i in tokens]\n",
    "\n",
    "print( \"porter: \" , porter_stemer)\n",
    "print(\"lancaster: \", lancaster_stemer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e86a3c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycountry'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycountry\u001b[39;00m\n\u001b[0;32m      5\u001b[0m countries \u001b[38;5;241m=\u001b[39m [country\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m pycountry\u001b[38;5;241m.\u001b[39mcountries]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(word):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycountry'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pycountry\n",
    "\n",
    "countries = [country.name for country in pycountry.countries]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert(word):\n",
    "    \"\"\"converts an adjectival nationality to its corresponding noun form.\"\"\"\n",
    "\n",
    "    # list of regex things to check\n",
    "patterns = ['ese', 'ian', 'an', 'ean', 'n', 'ic', 'ern']\n",
    "    #list of suffixes for appending to country names that get damaged when they are split.\n",
    "suffixes = ['a', 'o']\n",
    "\n",
    "    # for every potential way of forming a nationality adjective, test them.\n",
    "for pattern in patterns:\n",
    "    tup = re.findall(r'^(.*)(' + pattern + ')', word)\n",
    "\n",
    "    #if the regex finds a pattern, set the country to the stem of the word.\n",
    "\n",
    "if tup:\n",
    "    country = tup[0][0]\n",
    "\n",
    "# check to see if the country is in the list of countries returned by pycountry. If it is, return it.\n",
    "if country in countries:\n",
    "    return country\n",
    "\n",
    "# if the stem is not a country, try adding suffixes to it to see if you can pull out a real country.\n",
    "\n",
    "else:\n",
    "        for suffix in suffixes:\n",
    "            new_country = country + suffix\n",
    "            if new_country in countries:\n",
    "                return new_country\n",
    "\n",
    "print(convert('Mexican'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a73668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
