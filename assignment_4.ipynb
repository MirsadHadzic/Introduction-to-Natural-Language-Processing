{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61d6872b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.book import *\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a5ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca741a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find one website online and load content of it \n",
    "# Extract text from the content of website\n",
    "# Make sure to remove all unnecessary parts such as text from header, footer, nav bar, …\n",
    "# Find most frequently used words\n",
    "# Identify keywords for the selected website\n",
    "\n",
    "url = 'https://www.klix.ba/'  \n",
    "response = requests.get(url)\n",
    "html = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bdce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768e503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copyright 2000-2023 InterSoft d.o.o. Sarajevo. ISSN 2566-3771. Sva prava zadržana. Zabranjeno preuzimanje sadržaja bez dozvole izdavača.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62b03b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "footer_pattern = r'<footer>.*?</footer>'\n",
    "text = re.sub(footer_pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00977081",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfa7f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c434e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10564736",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(filtered_words)\n",
    "most_common_words = word_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7bf8969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words:\n",
      "copyright: 1\n",
      "intersoft: 1\n",
      "sarajevo: 1\n",
      "issn: 1\n",
      "sva: 1\n",
      "prava: 1\n",
      "zadržana: 1\n",
      "zabranjeno: 1\n",
      "preuzimanje: 1\n",
      "sadržaja: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Most common words:\")\n",
    "for word, freq in most_common_words:\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "275121f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keywords:\n",
      "['copyright', 'intersoft', 'sarajevo', 'issn', 'sva', 'prava', 'zadržana', 'zabranjeno', 'preuzimanje', 'sadržaja']\n"
     ]
    }
   ],
   "source": [
    "keywords = [word for word, freq in most_common_words]\n",
    "print(\"\\nKeywords:\")\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81dda7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words: [('domain', 3), ('use', 2), ('example', 1), ('illustrative', 1), ('examples', 1), ('documents', 1), ('may', 1), ('literature', 1), ('without', 1), ('prior', 1)]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def fetch_website_content(url):\n",
    "    # Fetch HTML content of the website\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    return html_content\n",
    "\n",
    "def extract_text(html_content):\n",
    "    # Use BeautifulSoup to parse HTML and extract text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Extract text from the main content, you might need to customize this based on the structure of the website\n",
    "    main_content = soup.find('body')  # Adjust based on the HTML structure\n",
    "    if main_content:\n",
    "        text = main_content.get_text(separator=' ')\n",
    "        return text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove unnecessary parts like header, footer, navigation bar, etc.\n",
    "    # You may need to customize this based on the structure of the website\n",
    "    # Example: Remove text inside script and style tags\n",
    "    text = ' '.join([word for word in text.split() if not word.startswith('<script') and not word.startswith('<style')])\n",
    "    return text\n",
    "\n",
    "def find_most_frequent_words(text, num_words=10):\n",
    "    # Tokenize the text and remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    freq_dist = FreqDist(filtered_tokens)\n",
    "\n",
    "    # Get the most frequent words\n",
    "    most_frequent_words = freq_dist.most_common(num_words)\n",
    "\n",
    "    return most_frequent_words\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the URL of the website you want to analyze\n",
    "    website_url = \"https://example.com\"\n",
    "\n",
    "    # Fetch website content\n",
    "    html_content = fetch_website_content(website_url)\n",
    "\n",
    "    # Extract text from the content\n",
    "    text = extract_text(html_content)\n",
    "\n",
    "    if text:\n",
    "        # Clean the text by removing unnecessary parts\n",
    "        cleaned_text = clean_text(text)\n",
    "\n",
    "        # Find the most frequent words\n",
    "        most_frequent_words = find_most_frequent_words(cleaned_text)\n",
    "\n",
    "        print(f\"Most frequent words: {most_frequent_words}\")\n",
    "    else:\n",
    "        print(\"Unable to extract text from the website.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81836ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
